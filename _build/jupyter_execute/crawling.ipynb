{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b708a56",
   "metadata": {},
   "source": [
    "## CRAWLING DATA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657edca0",
   "metadata": {},
   "source": [
    "Data adalah  kumpulan atau catatan fakta yang menggambarkan suatu kejadian, dan masih dalam bentuk data mentah sehingga perlu diolah lebih lanjut untuk menghasilkan informasi. Crawling adalah proses menjelajahi web dan mengunduh halaman web secara otomatis untuk mengumpulkan informasi. Program yang khusus bertugas melakukan crawling disebut Crawler. (Hanifah & Nurhasanah, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c415fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class Crawling(scrapy.Spider):\n",
    "    name = \"Crawling\"\n",
    "    \n",
    "    def start_requests(self):\n",
    "        x = 100000\n",
    "        for i in range (1,5):\n",
    "            x +=1\n",
    "            urls = [\n",
    "                'https://pta.trunojoyo.ac.id/welcome/detail/040411'+str(x),\n",
    "                'https://pta.trunojoyo.ac.id/welcome/detail/050411'+str(x),\n",
    "                'https://pta.trunojoyo.ac.id/welcome/detail/060411'+str(x),\n",
    "                'https://pta.trunojoyo.ac.id/welcome/detail/070411'+str(x),\n",
    "                'https://pta.trunojoyo.ac.id/welcome/detail/080411'+str(x),\n",
    "                'https://pta.trunojoyo.ac.id/welcome/detail/090411'+str(x),\n",
    "            ]\n",
    "            for url in urls:\n",
    "                yield scrapy.Request(url=url, callback=self.parse)\n",
    "    \n",
    "    def parse(self, response):\n",
    "        yield{\n",
    "            'judul' : response.css('#content_journal > ul > li > div:nth-child(2) > a').extract(),\n",
    "            'abstrak' : response.css('#content_journal > ul > li > div:nth-child(4) > div:nth-child(2) > p').extract()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7366a1",
   "metadata": {},
   "source": [
    "Digunakan untuk mengcrawl data dan hasil crawlnya dalam bentuk csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "143108dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_892/854070405.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Hp\\AppData\\Local\\Temp/ipykernel_892/854070405.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    scrapy crawl Crawling -o data.csv\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "scrapy crawl Crawling -o data.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}